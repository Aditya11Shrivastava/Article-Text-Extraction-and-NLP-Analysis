{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_excel(\"input.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load positive and negative word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"positive-words.txt\", \"r\") as f:\n",
    "    positive_words = f.read().splitlines()\n",
    "\n",
    "with open(\"negative-words.txt\", \"r\") as f:\n",
    "    negative_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stopwords_dir = \"StopWords\"  # here StopWords files are in a directory named \"StopWords\"\n",
    "\n",
    "for filename in os.listdir(stopwords_dir):\n",
    "    if filename.startswith(\"StopWords_\") and filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(stopwords_dir, filename), \"r\") as f:\n",
    "            stop_words.update(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find article title and text (this may need to be adjusted based on website structure)\n",
    "        title = soup.find('h1').get_text().strip()\n",
    "        article_content = soup.find('div', class_='td-post-content').get_text().strip()\n",
    "\n",
    "        return title + '\\n\\n' + article_content \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"{url_id}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):# silent 'e' at the end of a word does not contribute to syllable count. (common rule in english)\n",
    "        count -= 1\n",
    "    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels: # ex: word = able has 2 syllables.\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    if not text:  # Handle empty text case\n",
    "        return [0] * 13  # Return zeros for all variables\n",
    "\n",
    "    # Tokenize text\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Sentiment analysis\n",
    "    positive_score = sum([1 for word in cleaned_words if word in positive_words])\n",
    "    negative_score = sum([1 for word in cleaned_words if word in negative_words])\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "\n",
    "    # Readability\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    complex_word_count = sum([1 for word in cleaned_words if count_syllables(word) > 2])\n",
    "    percentage_complex_words = complex_word_count / len(cleaned_words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Other variables\n",
    "    word_count = len(cleaned_words)\n",
    "    syllable_per_word = ([count_syllables(word) for word in cleaned_words])\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))  # Count personal pronouns\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words) / word_count\n",
    "\n",
    "    return [\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "        avg_sentence_length, percentage_complex_words, fog_index,\n",
    "        avg_sentence_length,  # AVG NUMBER OF WORDS PER SENTENCE is the same as AVG SENTENCE LENGTH\n",
    "        complex_word_count, word_count, syllable_per_word,\n",
    "        personal_pronouns, avg_word_length\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackassign0036: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "blackassign0049: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Extract text and save to file\n",
    "    article_text = extract_article_text(url)\n",
    "    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(article_text)\n",
    "\n",
    "    # Analyze text\n",
    "    analysis_results = analyze_text(article_text)\n",
    "\n",
    "    # Combine input data and analysis results\n",
    "    output_data.append([url_id, url] + analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output_data, columns=[\n",
    "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE', 'AVG SENTENCE'   'LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction and analysis complete. Output saved to 'Output Data Structure.xlsx'\n"
     ]
    }
   ],
   "source": [
    "output_df.to_excel(\"Output Data Structure.xlsx\", index=False)\n",
    "\n",
    "print(\"Text extraction and analysis complete. Output saved to 'Output Data Structure.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
